{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5, Lab 2: Tackling an NLP Problem with Naive Bayes\n",
    "> Authors: Matt Brems, Caroline Schmitt\n",
    "\n",
    "We can sketch out the data science process as follows:\n",
    "1. Define the problem.\n",
    "2. Obtain the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "In this lab, we are going to apply a **new** modeling technique to natural language processing data.\n",
    "\n",
    "> \"But how can we apply a modeling technique we haven't learned?!\"\n",
    "\n",
    "The DSI program is great - but we can't teach you *everything* about data science in 12 weeks! This lab is designed to help you start learning something new without it being taught in a formal lesson. \n",
    "- Later in the cohort (like for your capstone!), you'll be exploring models, libraries, and resources that you haven't been explicitly taught.\n",
    "- After the program, you'll want to continue developing your skills. Being comfortable with documentation and being confident in your ability to read something new and decide whether or not it is an appropriate method for the problem you're trying to solve is **incredibly** valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define the problem.\n",
    "\n",
    "Many organizations have a substantial interest in classifying users of their product into groups. Some examples:\n",
    "- A company that serves as a marketplace may want to predict who is likely to purchase a certain type of product on their platform, like books, cars, or food.\n",
    "- An application developer may want to identify which individuals are willing to pay money for \"bonus features\" or to upgrade their app.\n",
    "- A social media organization may want to identify who generates the highest rate of content that later goes \"viral.\"\n",
    "\n",
    "### Summary\n",
    "In this lab, you're an engineer for Facebook. In recent years, the organization Cambridge Analytica gained worldwide notoriety for its use of Facebook data in an attempt to sway electoral outcomes.\n",
    "\n",
    "Cambridge Analytica, an organization staffed with lots of Ph.D. researchers, used the Big5 personality groupings (also called OCEAN) to group people into one of 32 different groups.\n",
    "- The five qualities measured by this personality assessment are:\n",
    "    - **O**penness\n",
    "    - **C**onscientiousness\n",
    "    - **E**xtroversion\n",
    "    - **A**greeableness\n",
    "    - **N**euroticism\n",
    "- Each person could be classified as \"Yes\" or \"No\" for each of the five qualities.\n",
    "- This makes for 32 different potential combinations of qualities. ($2^5 = 32$)\n",
    "- You don't have to check it out, but if you want to learn more about this personality assessment, head to [the Wikipedia page](https://en.wikipedia.org/wiki/Big_Five_personality_traits).\n",
    "- There's also [a short (3-4 pages) academic paper describing part of this approach](./celli-al_wcpr13.pdf).\n",
    "\n",
    "Cambridge Analytica's methodology was, roughly, the following:\n",
    "- Gather a large amount of data from Facebook.\n",
    "- Use this data to predict an individual's Big5 personality \"grouping.\"\n",
    "- Design political advertisements that would be particularly effective to that particular \"grouping.\" (For example, are certain advertisements particularly effective toward people with specific personality traits?)\n",
    "\n",
    "You want to know the **real-world problem**: \"Is what Cambridge Analytica attempted to do actually possible, or is it junk science?\"\n",
    "\n",
    "However, we'll solve the related **data science problem**: \"Are one's Facebook statuses predictive of whether or not one is agreeable?\"\n",
    "> Note: If Facebook statuses aren't predictive of one being agreeable (one of the OCEAN qualities), then Cambridge Analytica's approach won't work very well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Obtain the data.\n",
    "\n",
    "Obviously, there are plenty of opportunities to discuss the ethics surrounding this particular issue... so let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./mypersonality_final.csv', encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>DATE</th>\n",
       "      <th>NETWORKSIZE</th>\n",
       "      <th>BETWEENNESS</th>\n",
       "      <th>NBETWEENNESS</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>BROKERAGE</th>\n",
       "      <th>NBROKERAGE</th>\n",
       "      <th>TRANSITIVITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/19/09 03:21 PM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/02/09 08:41 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is sore and wants the knot of muscles at the b...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/15/09 01:15 PM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes how the day sounds in this new song.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/22/09 04:48 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is home. &lt;3</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/20/09 02:31 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #AUTHID  \\\n",
       "0  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "1  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "2  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "3  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "4  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "\n",
       "                                              STATUS  sEXT  sNEU  sAGR  sCON  \\\n",
       "0                        likes the sound of thunder.  2.65   3.0  3.15  3.25   \n",
       "1  is so sleepy it's not even funny that's she ca...  2.65   3.0  3.15  3.25   \n",
       "2  is sore and wants the knot of muscles at the b...  2.65   3.0  3.15  3.25   \n",
       "3         likes how the day sounds in this new song.  2.65   3.0  3.15  3.25   \n",
       "4                                        is home. <3  2.65   3.0  3.15  3.25   \n",
       "\n",
       "   sOPN cEXT cNEU cAGR cCON cOPN               DATE  NETWORKSIZE  BETWEENNESS  \\\n",
       "0   4.4    n    y    n    n    y  06/19/09 03:21 PM        180.0      14861.6   \n",
       "1   4.4    n    y    n    n    y  07/02/09 08:41 AM        180.0      14861.6   \n",
       "2   4.4    n    y    n    n    y  06/15/09 01:15 PM        180.0      14861.6   \n",
       "3   4.4    n    y    n    n    y  06/22/09 04:48 AM        180.0      14861.6   \n",
       "4   4.4    n    y    n    n    y  07/20/09 02:31 AM        180.0      14861.6   \n",
       "\n",
       "   NBETWEENNESS  DENSITY  BROKERAGE  NBROKERAGE  TRANSITIVITY  \n",
       "0         93.29     0.03    15661.0        0.49           0.1  \n",
       "1         93.29     0.03    15661.0        0.49           0.1  \n",
       "2         93.29     0.03    15661.0        0.49           0.1  \n",
       "3         93.29     0.03    15661.0        0.49           0.1  \n",
       "4         93.29     0.03    15661.0        0.49           0.1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports.\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the difference between anonymity and confidentiality? All else held equal, which tends to keep people safer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "- Anonymity is when one's identity cannot be connected back to their data. (For example, if we didn't collect any personally identifiable information when we gathered the data.)\n",
    "- Confidentiality is when one's identity _could_ be connected back to their data, but the person who collected/stored the data is keeping that private. (For example, if we gathered people's information and assigned them a unique ID, then only made information publicly available with the unique ID and not their name, this would be confidential.)\n",
    "- All else held equal, **anonymity** keeps individuals safer because it would be impossible to connect individuals to their data. (This is particularly important when we're dealing with data that could be sensitive or even dangerous if it were exposed.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Suppose that the \"unique identifier\" in the above data, the `#AUTHID`, is a randomly generated key so that it can never be connected back to the original poster. Have we guaranteed anonymity here? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: No, we have not. \n",
    "- The `STATUS` could contain information that could identify an individual.\n",
    "- Using measures like `NETWORKSIZE` and `DATE` contain information that could identify an individual. (For example, if you know someone has 180 friends and commonly posts during waking hours for someone in the Central time zone, it's possible to narrow down who that person is.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. As an engineer for Facebook, you recognize that user data will be used by Facebook and by other organizations - that won't change. However, what are at least three recommendations you would bring to your manager to improve how data is used and shared? Be as specific as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: _(Answers may vary.)_\n",
    "- Take steps toward anonymizing the data rather than keep it confidential. Consider not storing the time of the post or measures like the number of connections someone has. If there is personally identifiable information (PII) in statuses, consider blacking out the specific parts that are PII.\n",
    "- Choose to not store any sensitive data, regardless of whether or not it is anonymous/confidential. If there are statuses that discuss sensitive topics (e.g. sex, illegal activity, drug use), consider excluding that from the data collection process entirely. (Or, alternatively, require approval from an [institutional review board](https://en.wikipedia.org/wiki/Institutional_review_board) or internal ethics committee before collecting that data.\n",
    "- Apply [differential privacy techniques](http://theconversation.com/explainer-what-is-differential-privacy-and-how-can-it-protect-your-data-90686) so that only aggregate data is available and individual-level information is not compromised - even if hacked!\n",
    "    - \"The differential privacy model guarantees that even if someone has complete information about 99 of 100 people in a data set, they still cannot deduce the information about the final person.\" [Source](http://theconversation.com/explainer-what-is-differential-privacy-and-how-can-it-protect-your-data-90686)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data.\n",
    "\n",
    "- Note: For our $X$ variable, we will only use the `STATUS` variable. For our $Y$ variable, we will only use the `cAGR` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explore the data here.\n",
    "> We aren't explicitly asking you to do specific EDA here, but what EDA would you generally do with this data? Do the EDA you usually would, especially if you know what the goal of this analysis is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#AUTHID         0\n",
       "STATUS          0\n",
       "sEXT            0\n",
       "sNEU            0\n",
       "sAGR            0\n",
       "sCON            0\n",
       "sOPN            0\n",
       "cEXT            0\n",
       "cNEU            0\n",
       "cAGR            0\n",
       "cCON            0\n",
       "cOPN            0\n",
       "DATE            0\n",
       "NETWORKSIZE     0\n",
       "BETWEENNESS     0\n",
       "NBETWEENNESS    0\n",
       "DENSITY         0\n",
       "BROKERAGE       0\n",
       "NBROKERAGE      0\n",
       "TRANSITIVITY    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values.\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the very small amount of missing data, drop the missing values.\n",
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data up into X and y.\n",
    "X = data['STATUS']\n",
    "y = data['cAGR'].map(lambda x: 1 if x == 'y' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataa into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "# Instantiate CountVectorizer.\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Fit CountVectorizer to training data.\n",
    "cv.fit(X_train)\n",
    "\n",
    "# Transform training and testing data based on the fit CountVectorizer.\n",
    "X_train_cv = cv.transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with our words.\n",
    "words = pd.DataFrame(X_train_cv.todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the         3819\n",
       "to          3381\n",
       "is          2380\n",
       "and         2265\n",
       "of          1587\n",
       "in          1456\n",
       "it          1238\n",
       "for         1204\n",
       "my          1169\n",
       "you         1148\n",
       "propname     981\n",
       "that         874\n",
       "on           843\n",
       "with         714\n",
       "be           664\n",
       "me           621\n",
       "at           591\n",
       "this         550\n",
       "have         549\n",
       "so           542\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the most frequently used words.\n",
    "words.sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the lengths of the words.\n",
    "lengths_of_words = [len(each) for each in words.columns]\n",
    "plt.hist(lengths_of_words)\n",
    "plt.title('Histogram of Word Lengths');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the lengths of the posts.\n",
    "lengths_of_posts = [len(each) for each in X]\n",
    "plt.hist(lengths_of_posts)\n",
    "plt.title('Histogram of Post Lengths');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What is the difference between CountVectorizer and TFIDFVectorizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "- CountVectorizer takes text data and creates columns (**vectors**) that **count** the number of occurrences of a given token in a given document.\n",
    "- TFIDFVectorizer takes text data adn creates columns (**vectors**) that return a modified count of the number of occurrences of a given token in a given document, where this modified count is based on the **term frequency** and the **inverse document frequency**. It measures how common a word is in a given document relative to how common that same word is across all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What are stopwords?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Stopwords are very common words that often contribute no real understanding or value to the thing we are trying to predict. Examples of common stopwords might be \"the,\" \"of,\" and \"and.\" We can define a list of stopwords manually, or use preloaded lists of stopwords - but different packages will use different stopwords by default!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Give an example of when you might remove stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: In a sentiment analysis where we seek to understand whether people feel positively or negatively toward something, the frequency of words like \"in\" or \"at\" might not be predictive of our $Y$ variable. By removing stopwords, we can often speed up our model fitting and/or make room for words that are more predictive of $Y$. (Though if we have time/computer power, we should try including and excluding stopwords to see which performs better!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Give an example of when you might keep stopwords in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "- If we want to predict the century in which a poem is written, the presence of \"common\" words like \"the\" or \"of\" may be highly predictive of our $Y$ variable!\n",
    "- If we want to create a customer service chatbot that could interact with a customer and respond to written questions in a human-like way, properly using \"common\" words like \"the\" and \"of\" is very important... to leave these words out may mean humans aren't able to understand the responses from your chatbot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data.\n",
    "\n",
    "We are going to fit two types of models: a logistic regression and [a Naive Bayes classifier](https://scikit-learn.org/stable/modules/naive_bayes.html).\n",
    "\n",
    "**Reminder:** We will only use the feature `STATUS` to model `cAGR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to attempt to fit our models on sixteen sets of features:\n",
    "\n",
    "1. CountVectorizer with 100 features, with English stopwords removed and with an `ngram_range` that includes 1 and 2.\n",
    "2. CountVectorizer with 100 features, with English stopwords removed and with the default `ngram_range`.\n",
    "3. CountVectorizer with 100 features, with English stopwords kept in and with an `ngram_range` that includes 1 and 2.\n",
    "4. CountVectorizer with 100 features, with English stopwords kept in and with the default `ngram_range`.\n",
    "5. CountVectorizer with 500 features, with English stopwords removed and with an `ngram_range` that includes 1 and 2.\n",
    "6. CountVectorizer with 500 features, with English stopwords removed and with the default `ngram_range`.\n",
    "7. CountVectorizer with 500 features, with English stopwords kept in and with an `ngram_range` that includes 1 and 2.\n",
    "8. CountVectorizer with 500 features, with English stopwords kept in and with the default `ngram_range`.\n",
    "9. TFIDFVectorizer with 100 features, with English stopwords removed and with an `ngram_range` that includes 1 and 2.\n",
    "10. TFIDFVectorizer with 100 features, with English stopwords removed and with the default `ngram_range`.\n",
    "11. TFIDFVectorizer with 100 features, with English stopwords kept in and with an `ngram_range` that includes 1 and 2.\n",
    "12. TFIDFVectorizer with 100 features, with English stopwords kept in and with the default `ngram_range`.\n",
    "13. TFIDFVectorizer with 500 features, with English stopwords removed and with an `ngram_range` that includes 1 and 2.\n",
    "14. TFIDFVectorizer with 500 features, with English stopwords removed and with the default `ngram_range`.\n",
    "15. TFIDFVectorizer with 500 features, with English stopwords kept in and with an `ngram_range` that includes 1 and 2.\n",
    "16. TFIDFVectorizer with 500 features, with English stopwords kept in and with the default `ngram_range`.\n",
    "\n",
    "### 9. Rather than manually instantiating 16 different vectorizers, what `sklearn` class have we learned about that might make this easier? Use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Pipelines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch score with CountVectorized features on training set is 0.6223.\n",
      "\n",
      "GridSearch score with CountVectorized features on testing set is 0.547.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate pipeline.\n",
    "pipe_cv = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('lr', LogisticRegression(solver = 'lbfgs'))\n",
    "])\n",
    "\n",
    "# Define grid of parameters to GridSearch over.\n",
    "params_grid = {\n",
    "    'cv__max_features': [100, 500],\n",
    "    'cv__stop_words': ['english', None],\n",
    "    'cv__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "\n",
    "# GridSearch over pipeline with given grid of parameters.\n",
    "gs_cv = GridSearchCV(pipe_cv, params_grid, cv=5)\n",
    "\n",
    "# Fit model.\n",
    "gs_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f'GridSearch score with CountVectorized features on training set is {round(gs_cv.score(X_train, y_train), 4)}.')\n",
    "print()\n",
    "print(f'GridSearch score with CountVectorized features on testing set is {round(gs_cv.score(X_test, y_test), 4)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch score with TFIDFVectorized features on training set is 0.6201.\n",
      "\n",
      "GridSearch score with TFIDFVectorized features on testing set is 0.5458.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate pipeline.\n",
    "pipe_tf = Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Define grid of parameters to GridSearch over.\n",
    "params_grid = {\n",
    "    'tf__max_features': [100, 500],\n",
    "    'tf__stop_words': ['english', None],\n",
    "    'tf__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "\n",
    "# GridSearch over pipeline with given grid of parameters.\n",
    "gs_tf = GridSearchCV(pipe_tf, params_grid, cv=5)\n",
    "\n",
    "# Fit model.\n",
    "gs_tf.fit(X_train, y_train)\n",
    "\n",
    "print(f'GridSearch score with TFIDFVectorized features on training set is {round(gs_tf.score(X_train, y_train),4)}.')\n",
    "print()\n",
    "print(f'GridSearch score with TFIDFVectorized features on testing set is {round(gs_tf.score(X_test, y_test),4)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. What are some of the advantages of fitting a logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: _(Answers may vary.)_\n",
    "- Logistic regression models are fast to fit.\n",
    "- Logistic regression models allow for us to interpret coefficients and the association between individual independent variables and the dependent variable. (Though it takes some computation to interpret them.)\n",
    "- Logistic regression models are more widely used and thus better understood than other techniques.\n",
    "- Logistic regression models often perform on par with or better than more \"complex\"/unfamiliar machine learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Fit a logistic regression model and compare it to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.531128\n",
       "0    0.468872\n",
       "Name: cAGR, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check baseline performance.\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline is about 53.1% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab best estimator from gs_cv.\n",
    "lr_model = gs_cv.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_a...enalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model.\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy score for our logistic regression model is: 0.9157.\n"
     ]
    }
   ],
   "source": [
    "print(f'Training accuracy score for our logistic regression model is: {round(lr_model.score(X_train, y_train),4)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy score for our logistic regression model is: 0.5813.\n"
     ]
    }
   ],
   "source": [
    "print(f'Testing accuracy score for our logistic regression model is: {round(lr_model.score(X_test, y_test),4)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is highly overfit, but the accuracy on the testing set is still about 6% higher than the accuracy from the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Naive Bayes \n",
    "\n",
    "Naive Bayes is a classification technique that relies on probability to classify observations.\n",
    "- It's based on a probability rule called **Bayes' Theorem**... thus, \"**Bayes**.\"\n",
    "- It makes an assumption that isn't often met, so it's \"**naive**.\"\n",
    "\n",
    "Despite being a model that relies on a naive assumption, it often performs pretty well! (This is kind of like linear regression... we aren't always guaranteed homoscedastic errors in linear regression, but the model might still do a good job regardless.)\n",
    "- [Interested in details? Read more here if you want.](https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf)\n",
    "\n",
    "\n",
    "The [sklearn documentation](https://scikit-learn.org/stable/modules/naive_bayes.html) is here, but it can be intimidating. So, to quickly summarize the Bayes and Naive parts of the model...\n",
    "\n",
    "#### Bayes' Theorem\n",
    "If you've seen Bayes' Theorem, it relates the probability of $P(A|B)$ to $P(B|A)$. (Don't worry; we won't be doing any probability calculations by hand! However, you may want to refresh your memory on conditional probability from our earlier lessons if you forget what a conditional probability is.)\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Bayes' Theorem: } P(A|B) &=& \\frac{P(B|A)P(A)}{P(B)}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "- Let $A$ be that someone is \"agreeable,\" like the OCEAN category.\n",
    "- Let $B$ represent the words used in their Facebook post.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Bayes' Theorem: } P(A|B) &=& \\frac{P(B|A)P(A)}{P(B)} \\\\\n",
    "\\Rightarrow P(\\text{person is agreeable}|\\text{words in Facebook post}) &=& \\frac{P(\\text{words in Facebook post}|\\text{person is agreeable})P(\\text{person is agreeable})}{P(\\text{words in Facebook post})}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "We want to calculate the probability that someone is agreeable **given** the words that they used in their Facebook post! (Rather than calculating this probability by hand, this is done under the hood and we can just see the results by checking `.predict_proba()`.) However, this is exactly what our model is doing. We can (a.k.a. the model can) calculate the pieces on the right-hand side of the equation to give us a probability estimate of how likely someone is to be agreeable given their Facebook post.\n",
    "\n",
    "#### Naive Assumption\n",
    "\n",
    "If our goal is to estimate $P(\\text{person is agreeable}|\\text{words in Facebook post})$, that can be quite tricky.\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>Bonus: if you want to understand why that's complicated, click here.</summary>\n",
    "    \n",
    "- The event $\\text{\"words in Facebook post\"}$ is a complicated event to calculate.\n",
    "\n",
    "- If a Facebook post has 100 words in it, then the event $\\text{\"words in Facebook post\"} = \\text{\"word 1 is in the Facebook post\" and \"word 2 is in the Facebook post\" and }\\ldots \\text{ and \"word 100 is in the Facebook post\"}$.\n",
    "\n",
    "- To calculate the joint probability of all 100 words being in the Facebook post gets complicated pretty quickly. (Refer back to the probability notes on how to calculate the joint probability of two events if you want to see more.)\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "To simplify matters, we make an assumption: **we assume that all of our features are independent of one another.**\n",
    "\n",
    "In some contexts, this assumption might be realistic!\n",
    "\n",
    "### 12. Why would this assumption not be realistic with NLP data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: When working with NLP data, our features correspond to tokens (usually individual words). The way language works, certain words are more likely to follow other words, and certain words are less likely to follow other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite this assumption not being realistic with NLP data, we still use Naive Bayes pretty frequently.\n",
    "- It's a very fast modeling algorithm. (which is great especially when we have lots of features and/or lots of data!)\n",
    "- It is often an excellent classifier, outperforming more complicated models.\n",
    "\n",
    "There are three common types of Naive Bayes models: Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes.\n",
    "- How do we pick which of the three models to use? It depends on our $X$ variable.\n",
    "    - Bernoulli Naive Bayes is appropriate when our features are all 0/1 variables.\n",
    "        - [Bernoulli NB Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB)\n",
    "    - Multinomial Naive Bayes is appropriate when our features are variables that take on only positive integer counts.\n",
    "        - [Multinomial NB Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)\n",
    "    - Gaussian Naive Bayes is appropriate when our features are Normally distributed variables. (Realistically, though, we kind of use Gaussian whenever neither Bernoulli nor Multinomial works.)\n",
    "        - [Gaussian NB Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Suppose you CountVectorized your features. Which Naive Bayes model would be most appropriate to fit? Why? Fit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: We fit multinomial Naive Bayes because CountVectorizer gives us an integer count of words in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy score for our Multinomial Naive Bayes model is: 0.8667.\n",
      "\n",
      "Testing accuracy score for our Multinomial Naive Bayes model is: 0.6091.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Multinomial Naive Bayes model.\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Fit model.\n",
    "mnb.fit(X_train_cv, y_train)\n",
    "\n",
    "# Evaluate predictions.\n",
    "print(f'Training accuracy score for our Multinomial Naive Bayes model is: {round(mnb.score(X_train_cv, y_train),4)}.')\n",
    "print()\n",
    "print(f'Testing accuracy score for our Multinomial Naive Bayes model is: {round(mnb.score(X_test_cv, y_test),4)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Suppose you TFIDFVectorized your features. Which Naive Bayes model would be most appropriate to fit? Why? Fit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: We fit Gaussian Naive Bayes because it cannot be Bernoulli Naive Bayes (not just 1s/0s) and it cannot be Multinomial (not just integer counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy score for our Gaussian Naive Bayes model is: 0.8733.\n",
      "\n",
      "Testing accuracy score for our Gaussian Naive Bayes model is: 0.545.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate TFIDFVectorizer.\n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "# Fit vectorizer.\n",
    "tf.fit(X_train, y_train)\n",
    "\n",
    "# Transform training and testing sets.\n",
    "X_train_tf = tf.transform(X_train).todense()\n",
    "X_test_tf = tf.transform(X_test).todense()\n",
    "\n",
    "# Instantiate Gaussian Naive Bayes model.\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Fit model.\n",
    "gnb.fit(X_train_tf, y_train)\n",
    "\n",
    "# Evaluate predictions,\n",
    "print(f'Training accuracy score for our Gaussian Naive Bayes model is: {round(gnb.score(X_train_tf, y_train),4)}.')\n",
    "print()\n",
    "print(f'Testing accuracy score for our Gaussian Naive Bayes model is: {round(gnb.score(X_test_tf, y_test),4)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Compare the performance of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a baseline model would be about 0.5311.\n",
      "\n",
      "Training accuracy score for our logistic regression model is: 0.9157.\n",
      "\n",
      "Testing accuracy score for our logistic regression model is: 0.5813.\n",
      "\n",
      "Training accuracy score for our Multinomial Naive Bayes model is: 0.8667.\n",
      "\n",
      "Testing accuracy score for our Multinomial Naive Bayes model is: 0.6091.\n",
      "\n",
      "Training accuracy score for our Gaussian Naive Bayes model is: 0.8733.\n",
      "\n",
      "Testing accuracy score for our Gaussian Naive Bayes model is: 0.545.\n"
     ]
    }
   ],
   "source": [
    "print(f'The accuracy of a baseline model would be about {round(y_train.value_counts(normalize=True)[1],4)}.')\n",
    "print()\n",
    "print(f'Training accuracy score for our logistic regression model is: {round(lr_model.score(X_train, y_train),4)}.')\n",
    "print()\n",
    "print(f'Testing accuracy score for our logistic regression model is: {round(lr_model.score(X_test, y_test),4)}.')\n",
    "print()\n",
    "print(f'Training accuracy score for our Multinomial Naive Bayes model is: {round(mnb.score(X_train_cv, y_train),4)}.')\n",
    "print()\n",
    "print(f'Testing accuracy score for our Multinomial Naive Bayes model is: {round(mnb.score(X_test_cv, y_test),4)}.')\n",
    "print()\n",
    "print(f'Training accuracy score for our Gaussian Naive Bayes model is: {round(gnb.score(X_train_tf, y_train),4)}.')\n",
    "print()\n",
    "print(f'Testing accuracy score for our Gaussian Naive Bayes model is: {round(gnb.score(X_test_tf, y_test),4)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Our best model appears to be the Multinomial Naive Bayes model, with a testing accuracy of about 61% (nearly 8 points above the baseline or a 14.6% increase)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Even though we didn't explore the full extent of Cambridge Analytica's modeling, based on what we did here, how effective was their approach at using Facebook data to model agreeableness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Based on what we did here, we were able to model agreeableness a bit better than baseline, but probably not enough to call it a \"success.\" If our political campaigns hinged on us properly predicting whether or not one was agreeable **and** whether or not one was extroverted **and** whether or not one was open **and** whether or not one was neurotic **and** whether or not one was conscientious, we probably wouldn't do a very good job.\n",
    "- For example, if we were to get a 61% accuracy rate in all five categories (and we assume independence among our predictions in these five categories), then we'd expect to correctly classify someone in all five buckets about $0.61^5\\approx8.44\\%$ of the time. _(Spoiler alert: that's not very good.)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
